\chapter{Design and Implementation}\label{implement}

    This chapter shows the overview of application design.
    The MoSCoW statement will describe the functional requirements and the implementation details will be explained in many aspects,
    including Native library, social distancing detection process, parallelisation and processed video rendering.

    \section{MoSCoW Statement}
        The aim of this method is prioritising the requirements,
        which are determined from usability, cost, or performance.
        The prioriy is divided into 4 categories: must have, should have, could have, and won't have \cite{8342602}.
        Next, we describe the requirements of each of them.

        \subsection{Must have}
            \begin{itemize}
                \setlength\itemsep{1em}
                \item The application \textbf{must} be able to detect people in the given image or video.
                \item The application \textbf{must} be able to determine distancing between people in the given image or video.
                \item The application \textbf{must} save the processed image of video.
                \item The application \textbf{must} allow the user to choose image or video from device's storage.
            \end{itemize}

        \subsection{Should have}
            \begin{itemize}
                \item The application \textbf{should} be able to stream video from camera.
                \item The application \textbf{should} be able to show detected people on camera.
                \item The application \textbf{should} support parallel computing.
            \end{itemize}

        \subsection{Could have}
            \begin{itemize}
                \item The application \textbf{could} choose computation options between sequencial or parallelism.
                \item The application \textbf{could} support NEON techonology.
                \item The application \textbf{could} be able to process the given tasks in background.
            \end{itemize}
        \subsection{Won't have}
            \begin{itemize}
                \item The application \textbf{won't} detect other objects which are not human.
                \item The application \textbf{won't} support GPU computation.
            \end{itemize}

    \section{Interface Design}
        This application consists of three main processing functions: using video, using a picture and live from a camera.
        When a user opens the application, the user will navigate to the main menu, which can be seen in Figure \ref{appendix-b:mainMenu}.
        The user will be able to choose the processing function by clicking buttons.
        As can be seen in Figure \ref{appendix-b:filePicker}, video and picture button, the first two buttons, will navigate the user to the device's directory for choosing a file.
        The file will be processed in the background when a file is chosen, and the status bar will be updated regarding the progress.
        When processing is done, as it shows in Figure \ref{appendix-b:process}, video or image will be shown at the top of the application
        The detected human will be drawn a red rectangle if the detected human is breaching social distancing detection,
        while the detected human will be drawn a green rectangle if the detected human is retaining social distancing.
        In addition, the user is able to use the camera to determine social distancing in real-time.
        The user will navigate to the camera activity when clicks the rightmost button in the application, as can be seen in Figure \ref{appendix-b:camera}.

    \section{Preparation}
        Application is implemented on the Android Operating System,
        and the target Software Development Kit (SDK) version is set at level 29, namely Android Q.
        This application is compiled and built by Android Studio version 3.5.3,
        and CMake is used for compiling C++ library with C++ version 11.
        Furthermore, OpenCV version 4.4.0, which is built as a shared library, is used for image processing and object detection.
        For object detection model, two models are used: You Only Look Once \cite{YOLO-v3} and MobileNet SSD \cite{mobilenet} \cite{ssd}.

    \section{Java Native Interface}
        The implementation is divided into 3 layers which can be seen in Figure \ref{applicationLayer}.
            The first layer is a Java layer, which mainly interacts with the user,
            checks permissions, handles activity lifecycle, communicates with Java Native Interface (JNI), and loads native libraries.
                Native libraries are compiled and built into shared libraries by a Native Development Kit (NDK) \cite{NDK-COOKBOOK}.
            The second layer is JNI, which is written in C or C++.
                The task of the JNI layer is an intermediate connection between the Java layer and a C++ layer.
            The last layer is the C++ layer, which alternatively performs calculation tasks,
            including Deep Neural Network and distance calculation.

        \begin{figure}[!ht]
            \centering
            \includegraphics[width=3in]{images/chapter3/application-layers.png}
            \caption{Application Layers}
            \label{applicationLayer}
        \end{figure}

        However, Deep Neural Network and distance calculation can be implemented in all layers.
        According to the Android Developer Guide \cite{ANDROID-01},
        Native Development Kit (NDK) is recommended for compiling C and C++ code into the native library,
        which is able to achieve higher performance.
        Thus, executing both tasks in the JNI and C++ layer gains a better performance.
        Furthermore, there are two advantages to implementing JNI.
            The first advantage is reducing JNI calling. Performing both tasks in an application layer requires to call JNI methods many times,
                and this adds a cost overhead.
                Thus, manually implementing JNI reduces the number of JNI calls.
            The second advantage is memory management. C++ is able to access values in memory by using a pointer.
                Thus, values can be directly accessed without copying.

        Java and JNI communicate through native function, which is written in the Java layer,
        and the example of the code can be seen in Listing \ref{lst:nativeLib}.
        Memory addresses of the pre-processed frame in Mat format will be pass as a parameter through native function,
        and it will be converted from Java type to Native type as can be seen in Listing \ref{lst:convert}.
        Then, the given addresses will be converted back into Mat format.

\begin{lstlisting}[caption={Java Native Function},captionpos=b,label={lst:nativeLib}]
    public class NativeLib {
        static {
            System.loadLibrary("native-lib");
        }

        public native static void process(long imageAddr);
    }
\end{lstlisting}

\begin{lstlisting}[caption={C++ JNI Method},captionpos=b,label={lst:convert}]
    Java_com_jinkawin_dissertation_NativeLib_process(jlong matAddr){
        Mat &frame = *(Mat *) matAddr;
        ImageProcessor::process(frame);
    }
\end{lstlisting}

    \section{Social Distancing Detection}
        % Intro to detection
        There are three main processes that are implemented to determine social distancing violation from image and video, as shown in Figure \ref{systemOverview}.

        The first process is pre-processing the given image, video, or video stream.
        The given video will be extracted into an array of images.
        Then, images will be converted into Bitmap and Mat format with RGBA colour model respectively.
        After that, the colour will be converted, which depends on the detection model.
        As mentioned in Section 4.1, two models are used for detecting humans in the given picture and video: YOLO model and MobileNet SSD model \cite{tensorflow2015-whitepaper}, \cite{YOLO-v3}.
        Colour will be converted to RGB if the detection model is YOLO,
        while MobileNet SSD requires BGR colour model.

        \begin{figure}[!ht]
            \centering
            \includegraphics[width=4in]{images/chapter3/system-overview.png}
            \caption{Detection System Overview}
            \label{systemOverview}
        \end{figure}

        The second process is object detection.
        Detection models are set up and configured differently before processing images.
        First of all, YOLO is used with Darknet \cite{darknet}, which is an open-source neural network framework,
            while MobileNet SSD is used with Caffe framework \cite{caffe}.
        Then, the threshold will be set for determining the confidence score of the detected object.
            The confidence score threshold of YOLO model will be set to 0.5 or 50 per cent.
            In other words, the detected object will be rejected if YOLO model cannot guarantee that a detected object is human,
            and its confidence score is lower than 50 per cent.
            In contrast, the confidence score threshold of MobileNet SSD model will be set to 0.3 or 30 per cent.
            Because of MobileNet SSD model has a lower ability to detect an object, the confidence threshold is set lower.
        After models are set up and configured, the image will be processed in 5 steps.
        For the first step, the pre-processed image will be converted to input Blob by passing Mat image to $blobFromImage()$ function, and scale factor will be passed as a parameter.
            Blob will be used for the forward propagation in the neural network.
        Secondly, the blob will be passed to the network through $forward()$ function,
            and the network's output is a list of detected boxes with a label and a confidence score.
        Then, detected objects will be classified.
            Non-human objects will be removed by considering the label of the detected object.
        After that, a confidence score will be filtered by considering the threshold that is set in the configuration step.
        Finally, $NMSBoxes()$ performs non-maximum suppression, which will reduce overlapping detected boxes.

        The last process is determining social distancing.
        After a list of detected object boxes is filtered,
        the distance between each box will be calculated by using the formula which is based on Euclidean distance\footnote{an explanation was given in chapter \ref{sectionDistanceCalculation}}.
        If the value of the calculated distance is lower than the threshold,
        this means that the two people are too close, and they are breaking the social distancing rule.
        The application will change the box's colour to red.
        In contrast, if the value of the calculation is greater than the threshold,
        there is no breaching of social distancing rule, and the box's colour will be changed to green.

    \section{Parallelisation}
        Multithreading is used to reduce the processing time, which can achieve nearly real-time processing.
        The strategy of multithreading is dividing an input video into frames,
        and assign frames to threads by considering a number of available cores.

         There are two things that will be considered while the application is performing multithreading to avoid overheads.
        Firstly, Input/Output (I/O) operation must be avoided by threading.
        Secondly, all variables should be considered due to limited memory.
            Variables that consume a large space of heap will be initiated as static by using static block.
            In addition, the usage of the short-lived variables will be reduced to avoid garbage collection.
            Furthermore, a task will be recycled after a thread finished processing the given frame.

        \begin{figure}[!ht]
            \centering
            \includegraphics[width=4in]{images/chapter3/parallel.png}
            \caption{Parallel Computing Diagram}
            \label{parallelJava}
        \end{figure}

        In Java multithreading, a processor manager is implemented for managing threads efficiently.
        To ensure that the processor manager will be created at once,
        the processor manager is designed with singleton pattern and static block.
        Thread pool and queues are fundamental operators in the processor manager.
        As can be seen in Figure \ref{parallelJava}, frames in Mat format will be assigned as a task and queued in TaskQueue.
        Then, tasks will be mapped with thread orderly by the thread pool.
        A working thread will process the given task.
        After a thread finishs the given task,
        the thread will write a log regarding social distancing detection, and notify the processor manager.
        Then, a task will be recycled to free up memory.
        When the processor manager receives a notification from workers,
        a processed frame will be retrieved and ordered in the priority queue.
        After the process manager retrieves all processed frames,
        the process manager will notify the main activity.
        All of these processes are run in the background to avoid a frozen application.

        On the other hand, multithreading in C++ is slightly different, which can be seen in Listing \ref{lst:multithreadingC}.
        The central concept of multithreading is the same as Java, except thread management.
        A thread pool is implemented in Java to manage and handle threads,
        while parallelism framework in OpenCV is used in C++ \cite{opencv_library}.
        This framework is compiled with Threading Building Blocks (TBB), which is developed by Intel.

        \begin{figure}[!ht]
            \centering
            \includegraphics[width=3in]{images/chapter3/cpp-parallel.png}
            \caption{Multithreading in C++}
            \label{parallelCpp}
        \end{figure}

\begin{lstlisting}[caption={The example of multithreading in C++},captionpos=b,label={lst:multithreadingC}]
    virtual void operator()(const cv::Range& range) const{
        for (size_t i = range.start; i < size; i += threadNo){
            cv::Mat &frame = *(cv::Mat *) frames[i];
            ImageProcessor::process(frame);
        }
    }
\end{lstlisting}

        To reduce the processing time,
        the threads will process the frame which has the same index as itself,
        and the index will be increased by the number of total available cores.
        For example, as can be seen in Figure \ref{parallelCpp},
        the first thread will process the first frame and the fifth frame.
        Theoretically, if the frame rate of the video is 30 FPS, each frame should be processed within 33 miliseconds.
        Thus, if there are four threads,
        the first thread will have about 100 milliseconds to finish processing the first frame before moving to the fifth frame.

    \section{Frame Rendering}
        The maximum processing frame rate of this application is 10 FPS, which will be discussed in Chapter \ref{testing}.
        At this frame rate, video cannot be displayed smoothly in live camera;
        thus, there are steps that help video to be displayed in a higher frame rate.
        First of all, when a frame is streamed from the camera,
        the frame will be considered to be processed or not be processed.
        Processing will be determined from the capability of the device.
        In other words, the maximum processing frame rate will be used as a threshold.
        If a number of processed frames in 1 second exceeds the threshold,
        the frame will not be sent to the processor manager.
        After that, the main activity will check a processed frame in a priority queue from processor manager.
        If there is a processed frame in the priority queue, the processed frame will be displayed to the screen.
        Otherwise, the main activity will read the last log of the processed frame and retrieve all detection rectangles.
        Then, the main activity will draw those rectangles on the incoming frame and display it on the screen.
